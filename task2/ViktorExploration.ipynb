{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import os\n",
    "\n",
    "import biosppy.signals.ecg as ecg\n",
    "import neurokit2 as nk\n",
    "import tsfel\n",
    "import emd\n",
    "import tqdm\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler, MinMaxScaler\n",
    "import tensorflow as tf\n",
    "import IPython.display as ipd\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy as sp\n",
    "from numpy.fft import rfft, irfft\n",
    "from tqdm import tqdm\n",
    "import pywt\n",
    "\n",
    "import keras\n",
    "\n",
    "from tensorflow.keras.layers import Dense, Flatten, Dropout\n",
    "from tensorflow.keras.models import Sequential\n",
    "\n",
    "from sklearn.impute import KNNImputer\n",
    "\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "from scipy import stats\n",
    "from statistics import pstdev,variance\n",
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import balanced_accuracy_score\n",
    "\n",
    "from sktime.transformations.panel.rocket import MiniRocket, MiniRocketMultivariateVariable\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, make_scorer, f1_score\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = pd.read_csv(\"task2/X_train.csv\").drop('id', axis=1)\n",
    "y_train = pd.read_csv(\"task2/y_train.csv\").drop('id', axis=1)\n",
    "X_test = pd.read_csv(\"task2/X_test.csv\").drop('id', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "(16322,)\n",
      "9.16431809827227\n"
     ]
    }
   ],
   "source": [
    "dataset_x = np.array(X_train)\n",
    "for i in range(1):\n",
    "    signal = dataset_x[i]\n",
    "    signal = signal[~np.isnan(signal)]\n",
    "    signal_serie = pd.Series(signal)\n",
    "    corr = signal_serie.autocorr(lag=2)\n",
    "    print(type(signal))\n",
    "    print(signal.shape)\n",
    "    print(np.average(signal))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_extraction(dataset_x):\n",
    "\n",
    "    #compute some statistics of all the signals \n",
    "    autocor = []\n",
    "    ptp = []\n",
    "    med = []\n",
    "    avg = []\n",
    "    fft=[]\n",
    "\n",
    "    n_rows = dataset_x.shape[0]\n",
    "    n_cols = dataset_x.shape[1]\n",
    "\n",
    "    dataset_x = np.array(dataset_x)\n",
    "\n",
    "    for i in range(n_rows):\n",
    "        signal = dataset_x[i]\n",
    "        signal = signal[~np.isnan(signal)]\n",
    "\n",
    "        signal_serie = pd.Series(signal)\n",
    "        corr = signal_serie.autocorr(lag=2)\n",
    "        autocor.append(corr)\n",
    "        #average, range, median\n",
    "        avg.append(np.average(signal))\n",
    "        ptp.append(np.ptp(signal))\n",
    "        med.append(np.median(signal))\n",
    "        #top 20 fft frequencies\n",
    "        f = np.fft.fft(signal)\n",
    "        array = f[0:800]\n",
    "        n = 20\n",
    "        indices = array.argsort()[-n:][::-1]\n",
    "        fft.append(indices)\n",
    "\n",
    "\n",
    "    #Padding the data in case some library need, padding with the last value. \n",
    "    to_pad = n_cols\n",
    "    new_seq = []\n",
    "    for one_seq in dataset_x:\n",
    "        one_seq = one_seq[~np.isnan(one_seq)]\n",
    "        len_one_seq = len(one_seq)\n",
    "        last_val = one_seq[-1]\n",
    "        n = to_pad - len_one_seq\n",
    "        to_concat = np.repeat(0, n)\n",
    "        new_one_seq = np.concatenate([one_seq, to_concat])\n",
    "        new_seq.append(new_one_seq)\n",
    "    padded = np.stack(new_seq)\n",
    "    padded.shape\n",
    "\n",
    "    #templates_ts: equal step timestep, axis reference\n",
    "    #templates: #heartbeat * timestep \n",
    "    #heart_rate_ts: heart_rate_ts, equal-step timestamp\n",
    "    #heart_rate: heart rate at each time\n",
    "\n",
    "    ts_list = []\n",
    "    filtered_list=[]\n",
    "    rpeaks_list=[]\n",
    "    templates_ts_list=[]\n",
    "    templates_list=[]\n",
    "    heart_rate_ts_list=[]\n",
    "    heart_rate_list=[]\n",
    "    norm_average_heartbeat_list = [] #normalized average heartbeat of pacient\n",
    "    for i in range(n_rows):\n",
    "        ts, filtered, rpeaks, templates_ts, templates, heart_rate_ts, heart_rate = ecg.ecg(signal=padded[i], sampling_rate=300.0, show=False)\n",
    "        ts_list.append(ts)\n",
    "        filtered_list.append(filtered)\n",
    "        rpeaks_list.append(rpeaks)\n",
    "        templates_ts_list.append(templates_ts)\n",
    "        templates_list.append(templates)\n",
    "        heart_rate_ts_list.append(heart_rate_ts)\n",
    "        heart_rate_list.append(heart_rate)\n",
    "        norm_template = normalize(templates)\n",
    "        norm_average_heartbeat_list.append(sum(norm_template)/len(norm_template))\n",
    "\n",
    "    # Energy of the signal\n",
    "    energy_list = []\n",
    "    for i in range(len(norm_average_heartbeat_list)):\n",
    "        energy_list.append(np.sum(norm_average_heartbeat_list[i] ** 2))   \n",
    "\n",
    "\n",
    "    # Extarct all the peaks \n",
    "    P_list=[]\n",
    "    Q_list=[]\n",
    "    R_list=[]\n",
    "    S_list=[]\n",
    "    T_list=[]\n",
    "    for i in range(len(norm_average_heartbeat_list)):\n",
    "        patient_current = norm_average_heartbeat_list[i]\n",
    "        # Find the peak\n",
    "        index = np.where(patient_current==max(patient_current))\n",
    "        R = index[0]\n",
    "        # First-half\n",
    "        first_half = patient_current[0:R[0]]\n",
    "        index = np.where(patient_current==min(first_half[R[0]-30:R[0]]))\n",
    "        Q = index[0]\n",
    "        index = np.where(first_half[0:Q[0]]==max(first_half[0:Q[0]]))\n",
    "        P = index[0]\n",
    "        #Second half\n",
    "        second_half = patient_current[R[0]+1:] \n",
    "        index = np.where(patient_current==min(second_half[0:30]))\n",
    "        S = index[0]\n",
    "        second_half = second_half[S[0]-R[0]+1:]\n",
    "        index = np.where(patient_current==max(second_half))\n",
    "        T = index[0] \n",
    "        P_list.append(P[0])\n",
    "        Q_list.append(Q[0])\n",
    "        R_list.append(R[0])\n",
    "        S_list.append(S[0])\n",
    "        T_list.append(T[0])\n",
    "\n",
    "    # Intervals and Ratios of peaks\n",
    "    PR_list = []\n",
    "    QRS_list = []\n",
    "    ST_list = []\n",
    "    for i in range(len(P_list)):\n",
    "        PR_list.append(R_list[i]-P_list[i])\n",
    "        QRS_list.append(S_list[i]-Q_list[i])\n",
    "        ST_list.append(T_list[i]-S_list[i])\n",
    "\n",
    "    #statistics of the rpeaks\n",
    "    rpeaks_mean_list=[]\n",
    "    rpeaks_median_list=[]\n",
    "    rpeaks_var_list = []\n",
    "    for i in range(len(rpeaks_list)):\n",
    "        rpeaks_mean_list.append(np.mean(rpeaks_list[i]))\n",
    "        rpeaks_median_list.append(np.median(rpeaks_list[i]))\n",
    "        rpeaks_var_list.append(np.var(rpeaks_list[i]))\n",
    "\n",
    "    #statistics of heartrate and heartbeat\n",
    "    max_A=[]\n",
    "    min_A=[]\n",
    "    mean_A=[]\n",
    "    median_A=[]\n",
    "\n",
    "    for i in range(len(norm_average_heartbeat_list)):\n",
    "        patient_current = norm_average_heartbeat_list[i]\n",
    "        max_A.append(max(patient_current))\n",
    "        min_A.append(min(patient_current))\n",
    "        mean_A.append(np.mean(patient_current))\n",
    "        median_A.append(np.median(patient_current))\n",
    "\n",
    "\n",
    "    hr_mean_list = []\n",
    "    hr_median_list = []\n",
    "    hr_var_list = []\n",
    "    for i in range(len(heart_rate_list)):\n",
    "            hr_mean_list.append(np.mean(heart_rate_list[i]))\n",
    "            hr_median_list.append(np.median(heart_rate_list[i]))\n",
    "            hr_var_list.append(np.var(heart_rate_list[i]))\n",
    "    hb_mean_list = []\n",
    "    hb_median_list = []\n",
    "    hb_var_list = []\n",
    "    for i in range(len(norm_average_heartbeat_list)):\n",
    "            hb_mean_list.append(np.mean(norm_average_heartbeat_list[i]))\n",
    "            hb_median_list.append(np.median(norm_average_heartbeat_list[i]))\n",
    "            hb_var_list.append(np.var(norm_average_heartbeat_list[i]))\n",
    "\n",
    "\n",
    "\n",
    "    #calculate the statistics of difference of heartrate\n",
    "    hr_diff_mean_list=[]\n",
    "    hr_diff_median_list=[]\n",
    "    hr_diff_var_list=[]\n",
    "    for i in range(len(heart_rate_list)):\n",
    "        d =np.diff(heart_rate_list[i])\n",
    "        hr_diff_mean_list.append(np.mean(d))\n",
    "        hr_diff_median_list.append(np.median(d))\n",
    "        hr_diff_var_list.append(np.var(d))    \n",
    "\n",
    "    # calculate the statistics of difference heartbeats\n",
    "    hb_diff_mean_list=[]\n",
    "    hb_diff_median_list=[]\n",
    "    hb_diff_var_list=[]\n",
    "    for i in range(len(templates_list)):\n",
    "        d =np.diff(templates_list[i])\n",
    "        hb_diff_mean_list.append(np.mean(d))\n",
    "        hb_diff_median_list.append(np.median(d))  \n",
    "        hb_diff_var_list.append(np.var(d))\n",
    "\n",
    "    cA_list=[]\n",
    "    cD_list=[]\n",
    "\n",
    "    for i in range(len(norm_average_heartbeat_list)):\n",
    "        cA, cD = pywt.dwt(norm_average_heartbeat_list[i], 'db2', mode='periodic')\n",
    "        cA_list.append(cA)\n",
    "        cD_list.append(cD)\n",
    "\n",
    "\n",
    "    np_autocor = np.array([autocor]).reshape(-1,1)\n",
    "    np_ptp = np.array([ptp]).reshape(-1,1)\n",
    "    np_med = np.array([med]).reshape(-1,1)\n",
    "    np_avg = np.array([avg]).reshape(-1,1)\n",
    "    np_fft = np.array(fft)\n",
    "    np_energy =np.array(energy_list).reshape(-1,1)\n",
    "\n",
    "\n",
    "    np_P = np.array(P_list).reshape(-1,1)\n",
    "    np_Q = np.array(Q_list).reshape(-1,1)\n",
    "    np_R = np.array(R_list).reshape(-1,1)\n",
    "    np_S = np.array(S_list).reshape(-1,1)\n",
    "    np_T = np.array(T_list).reshape(-1,1)\n",
    "\n",
    "    np_PR = np.array(PR_list).reshape(-1,1)\n",
    "    np_QRS = np.array(QRS_list).reshape(-1,1)\n",
    "    np_ST = np.array(ST_list).reshape(-1,1)\n",
    "\n",
    "    np_QRS_T= np.divide(QRS_list, T_list) \n",
    "    np_QRS_P= np.divide(QRS_list, P_list) \n",
    "    np_QRS_T=np.nan_to_num(np_QRS_T, nan=0.0,posinf=0.0, neginf=0.0).reshape(-1,1)\n",
    "    np_QRS_P=np.nan_to_num(np_QRS_P, nan=0.0,posinf=0.0, neginf=0.0).reshape(-1,1)\n",
    "\n",
    "    np_rpeaks_mean = np.array(rpeaks_mean_list).reshape(-1,1)\n",
    "    np_rpeaks_median = np.array(rpeaks_median_list).reshape(-1,1)\n",
    "    np_rpeaks_var = np.array(rpeaks_var_list).reshape(-1,1)\n",
    "\n",
    "    max_A=np.array(max_A).reshape(-1,1)\n",
    "    min_A=np.array(min_A).reshape(-1,1)\n",
    "    mean_A=np.array(mean_A).reshape(-1,1)\n",
    "    median_A=np.array(median_A).reshape(-1,1)\n",
    "\n",
    "    np_hr_mean = np.array(hr_mean_list).reshape(-1,1)\n",
    "    np_hr_median = np.array(hr_median_list).reshape(-1,1)\n",
    "    np_hr_var = np.array(hr_var_list).reshape(-1,1)\n",
    "    \n",
    "    nans_hr = np_hr_mean.isna().sum()\n",
    "    print(\"is there Nan:\",nans_hr)\n",
    "\n",
    "    np_hb_mean = np.array(hb_mean_list).reshape(-1,1)\n",
    "    np_hb_median = np.array(hb_median_list).reshape(-1,1)\n",
    "    np_hb_var = np.array(hb_var_list).reshape(-1,1)   \n",
    "\n",
    "    np_hr_diff_mean = np.array(hr_diff_mean_list).reshape(-1,1)\n",
    "    np_hr_diff_median = np.array(hr_diff_median_list).reshape(-1,1)\n",
    "    np_hr_diff_var = np.array(hr_diff_var_list).reshape(-1,1)\n",
    "    np_hb_diff_mean = np.array(hb_diff_mean_list).reshape(-1,1)\n",
    "    np_hb_diff_median = np.array(hb_diff_median_list).reshape(-1,1)\n",
    "    np_hb_diff_var = np.array(hb_diff_var_list).reshape(-1,1)\n",
    "\n",
    "        \n",
    "\n",
    "    feature_set = np.concatenate((    \n",
    "        np_autocor,\n",
    "        np_ptp,\n",
    "        np_med,\n",
    "        np_avg,\n",
    "        np_fft,\n",
    "        np_energy,\n",
    "        \n",
    "        np_P,\n",
    "        np_Q,\n",
    "        np_R,\n",
    "        np_S,\n",
    "        np_T,\n",
    "\n",
    "        np_PR,\n",
    "        np_QRS,\n",
    "        np_ST,\n",
    "        np_QRS_T,\n",
    "        np_QRS_P,   \n",
    "\n",
    "        np_rpeaks_mean,\n",
    "        np_rpeaks_median,\n",
    "        np_rpeaks_var,\n",
    "\n",
    "        max_A-min_A,\n",
    "        mean_A,\n",
    "        median_A,\n",
    "\n",
    "        np_hr_mean,\n",
    "        np_hr_median,\n",
    "        np_hr_var,\n",
    "        np_hb_mean,\n",
    "        np_hb_median,\n",
    "        np_hb_var,   \n",
    "\n",
    "        np_hr_diff_mean,\n",
    "        np_hr_diff_median,\n",
    "        np_hr_diff_var,\n",
    "        np_hb_diff_mean,\n",
    "        np_hb_diff_median,\n",
    "        np_hb_diff_var,\n",
    "        cA_list,\n",
    "        cD_list   \n",
    "\n",
    "    ),axis = 1)\n",
    "\n",
    "    print(feature_set.shape)\n",
    "    #rocket features\n",
    "    feature_extractor_rocket = MiniRocket(random_state=1, num_kernels = 100)\n",
    "    rocket_features = feature_extractor_rocket.fit_transform(padded.reshape(padded.shape[0],1,padded.shape[1]))\n",
    "\n",
    "    #tsfel features\n",
    "    cfg = tsfel.get_features_by_domain('temporal')  # Or 'spectral' or 'statistical'\n",
    "    extracted_features = []\n",
    "    for i in range(padded.shape[0]):\n",
    "        features = tsfel.time_series_features_extractor(cfg, padded[i], fs=300,verbose=0)\n",
    "        extracted_features.append(features)\n",
    "    tsfel_features = pd.concat(extracted_features, ignore_index=True)\n",
    "\n",
    "    feature_set = pd.DataFrame(feature_set)\n",
    "\n",
    "    concatenated_features = pd.concat([feature_set,rocket_features,tsfel_features],axis=1)\n",
    "\n",
    "\n",
    "    return concatenated_features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_features = feature_extraction(X_train)\n",
    "#X_test_features = feature_extraction(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NaNs count by column:\n",
      "0                            0\n",
      "1                            0\n",
      "2                            0\n",
      "3                            0\n",
      "4                            0\n",
      "                            ..\n",
      "0_Positive turning points    0\n",
      "0_Signal distance            0\n",
      "0_Slope                      0\n",
      "0_Sum absolute diff          0\n",
      "0_Zero crossing rate         0\n",
      "Length: 333, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "nans_by_column = X_train_features.isna().sum()\n",
    "\n",
    "print(\"NaNs count by column:\")\n",
    "print(nans_by_column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " X_test_features.isna().sum().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0                            0\n",
      "1                            0\n",
      "2                            0\n",
      "3                            0\n",
      "4                            0\n",
      "5                            0\n",
      "6                            0\n",
      "7                            0\n",
      "8                            0\n",
      "9                            0\n",
      "10                           0\n",
      "11                           0\n",
      "12                           0\n",
      "13                           0\n",
      "14                           0\n",
      "15                           0\n",
      "16                           0\n",
      "17                           0\n",
      "18                           0\n",
      "19                           0\n",
      "20                           0\n",
      "21                           0\n",
      "22                           0\n",
      "23                           0\n",
      "24                           0\n",
      "25                           0\n",
      "26                           0\n",
      "27                           0\n",
      "28                           0\n",
      "29                           0\n",
      "30                           0\n",
      "31                           0\n",
      "32                           0\n",
      "33                           0\n",
      "34                           0\n",
      "35                           0\n",
      "36                           0\n",
      "37                           0\n",
      "38                           0\n",
      "39                           0\n",
      "40                           0\n",
      "41                           2\n",
      "42                           2\n",
      "43                           2\n",
      "44                           0\n",
      "45                           0\n",
      "46                           0\n",
      "47                           2\n",
      "48                           2\n",
      "49                           2\n",
      "50                           0\n",
      "51                           0\n",
      "52                           0\n",
      "53                           0\n",
      "54                           0\n",
      "55                           0\n",
      "56                           0\n",
      "57                           0\n",
      "58                           0\n",
      "59                           0\n",
      "60                           0\n",
      "61                           0\n",
      "62                           0\n",
      "63                           0\n",
      "64                           0\n",
      "65                           0\n",
      "66                           0\n",
      "67                           0\n",
      "68                           0\n",
      "69                           0\n",
      "70                           0\n",
      "71                           0\n",
      "72                           0\n",
      "73                           0\n",
      "74                           0\n",
      "75                           0\n",
      "76                           0\n",
      "77                           0\n",
      "78                           0\n",
      "79                           0\n",
      "80                           0\n",
      "81                           0\n",
      "82                           0\n",
      "83                           0\n",
      "84                           0\n",
      "85                           0\n",
      "86                           0\n",
      "87                           0\n",
      "88                           0\n",
      "89                           0\n",
      "90                           0\n",
      "91                           0\n",
      "92                           0\n",
      "93                           0\n",
      "94                           0\n",
      "95                           0\n",
      "96                           0\n",
      "97                           0\n",
      "98                           0\n",
      "99                           0\n",
      "100                          0\n",
      "101                          0\n",
      "102                          0\n",
      "103                          0\n",
      "104                          0\n",
      "105                          0\n",
      "106                          0\n",
      "107                          0\n",
      "108                          0\n",
      "109                          0\n",
      "110                          0\n",
      "111                          0\n",
      "112                          0\n",
      "113                          0\n",
      "114                          0\n",
      "115                          0\n",
      "116                          0\n",
      "117                          0\n",
      "118                          0\n",
      "119                          0\n",
      "120                          0\n",
      "121                          0\n",
      "122                          0\n",
      "123                          0\n",
      "124                          0\n",
      "125                          0\n",
      "126                          0\n",
      "127                          0\n",
      "128                          0\n",
      "129                          0\n",
      "130                          0\n",
      "131                          0\n",
      "132                          0\n",
      "133                          0\n",
      "134                          0\n",
      "135                          0\n",
      "136                          0\n",
      "137                          0\n",
      "138                          0\n",
      "139                          0\n",
      "140                          0\n",
      "141                          0\n",
      "142                          0\n",
      "143                          0\n",
      "144                          0\n",
      "145                          0\n",
      "146                          0\n",
      "147                          0\n",
      "148                          0\n",
      "149                          0\n",
      "150                          0\n",
      "151                          0\n",
      "152                          0\n",
      "153                          0\n",
      "154                          0\n",
      "155                          0\n",
      "156                          0\n",
      "157                          0\n",
      "158                          0\n",
      "159                          0\n",
      "160                          0\n",
      "161                          0\n",
      "162                          0\n",
      "163                          0\n",
      "164                          0\n",
      "165                          0\n",
      "166                          0\n",
      "167                          0\n",
      "168                          0\n",
      "169                          0\n",
      "170                          0\n",
      "171                          0\n",
      "172                          0\n",
      "173                          0\n",
      "174                          0\n",
      "175                          0\n",
      "176                          0\n",
      "177                          0\n",
      "178                          0\n",
      "179                          0\n",
      "180                          0\n",
      "181                          0\n",
      "182                          0\n",
      "183                          0\n",
      "184                          0\n",
      "185                          0\n",
      "186                          0\n",
      "187                          0\n",
      "188                          0\n",
      "189                          0\n",
      "190                          0\n",
      "191                          0\n",
      "192                          0\n",
      "193                          0\n",
      "194                          0\n",
      "195                          0\n",
      "196                          0\n",
      "197                          0\n",
      "198                          0\n",
      "199                          0\n",
      "200                          0\n",
      "201                          0\n",
      "202                          0\n",
      "203                          0\n",
      "204                          0\n",
      "205                          0\n",
      "206                          0\n",
      "207                          0\n",
      "208                          0\n",
      "209                          0\n",
      "210                          0\n",
      "211                          0\n",
      "212                          0\n",
      "213                          0\n",
      "214                          0\n",
      "215                          0\n",
      "216                          0\n",
      "217                          0\n",
      "218                          0\n",
      "219                          0\n",
      "220                          0\n",
      "221                          0\n",
      "222                          0\n",
      "223                          0\n",
      "224                          0\n",
      "225                          0\n",
      "226                          0\n",
      "227                          0\n",
      "228                          0\n",
      "229                          0\n",
      "230                          0\n",
      "231                          0\n",
      "232                          0\n",
      "233                          0\n",
      "234                          0\n",
      "0                            0\n",
      "1                            0\n",
      "2                            0\n",
      "3                            0\n",
      "4                            0\n",
      "5                            0\n",
      "6                            0\n",
      "7                            0\n",
      "8                            0\n",
      "9                            0\n",
      "10                           0\n",
      "11                           0\n",
      "12                           0\n",
      "13                           0\n",
      "14                           0\n",
      "15                           0\n",
      "16                           0\n",
      "17                           0\n",
      "18                           0\n",
      "19                           0\n",
      "20                           0\n",
      "21                           0\n",
      "22                           0\n",
      "23                           0\n",
      "24                           0\n",
      "25                           0\n",
      "26                           0\n",
      "27                           0\n",
      "28                           0\n",
      "29                           0\n",
      "30                           0\n",
      "31                           0\n",
      "32                           0\n",
      "33                           0\n",
      "34                           0\n",
      "35                           0\n",
      "36                           0\n",
      "37                           0\n",
      "38                           0\n",
      "39                           0\n",
      "40                           0\n",
      "41                           0\n",
      "42                           0\n",
      "43                           0\n",
      "44                           0\n",
      "45                           0\n",
      "46                           0\n",
      "47                           0\n",
      "48                           0\n",
      "49                           0\n",
      "50                           0\n",
      "51                           0\n",
      "52                           0\n",
      "53                           0\n",
      "54                           0\n",
      "55                           0\n",
      "56                           0\n",
      "57                           0\n",
      "58                           0\n",
      "59                           0\n",
      "60                           0\n",
      "61                           0\n",
      "62                           0\n",
      "63                           0\n",
      "64                           0\n",
      "65                           0\n",
      "66                           0\n",
      "67                           0\n",
      "68                           0\n",
      "69                           0\n",
      "70                           0\n",
      "71                           0\n",
      "72                           0\n",
      "73                           0\n",
      "74                           0\n",
      "75                           0\n",
      "76                           0\n",
      "77                           0\n",
      "78                           0\n",
      "79                           0\n",
      "80                           0\n",
      "81                           0\n",
      "82                           0\n",
      "83                           0\n",
      "0_Area under the curve       0\n",
      "0_Autocorrelation            0\n",
      "0_Centroid                   0\n",
      "0_Mean absolute diff         0\n",
      "0_Mean diff                  0\n",
      "0_Median absolute diff       0\n",
      "0_Median diff                0\n",
      "0_Negative turning points    0\n",
      "0_Neighbourhood peaks        0\n",
      "0_Positive turning points    0\n",
      "0_Signal distance            0\n",
      "0_Slope                      0\n",
      "0_Sum absolute diff          0\n",
      "0_Zero crossing rate         0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Assuming 'series' is your pandas Series\n",
    "\n",
    "# Set option to display maximum rows\n",
    "pd.set_option('display.max_rows', None)\n",
    "\n",
    "# Print the entire Series\n",
    "print(nans_by_column)\n",
    "\n",
    "# Reset the option back to default\n",
    "pd.reset_option('display.max_rows')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\zheng\\anaconda3\\Lib\\site-packages\\sklearn\\impute\\_base.py:555: UserWarning: Skipping features without any observed values: ['1' '2' '3']. At least one non-missing value is needed for imputation with strategy='median'.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Shape of passed values is (3411, 330), indices imply (3411, 333)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\zheng\\Desktop\\ETH Third Semester\\AML\\AML projects\\Second\\ViktorExploration.ipynb Cell 9\u001b[0m line \u001b[0;36m8\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/zheng/Desktop/ETH%20Third%20Semester/AML/AML%20projects/Second/ViktorExploration.ipynb#X43sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m imputer \u001b[39m=\u001b[39m SimpleImputer(strategy\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mmedian\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/zheng/Desktop/ETH%20Third%20Semester/AML/AML%20projects/Second/ViktorExploration.ipynb#X43sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m X_train_features \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mDataFrame(imputer\u001b[39m.\u001b[39mfit_transform(X_train_features), columns\u001b[39m=\u001b[39mX_train_features\u001b[39m.\u001b[39mcolumns)\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/zheng/Desktop/ETH%20Third%20Semester/AML/AML%20projects/Second/ViktorExploration.ipynb#X43sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m X_test_features \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mDataFrame(imputer\u001b[39m.\u001b[39mfit_transform(X_test_features), columns\u001b[39m=\u001b[39mX_train_features\u001b[39m.\u001b[39mcolumns)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/zheng/Desktop/ETH%20Third%20Semester/AML/AML%20projects/Second/ViktorExploration.ipynb#X43sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m y_train \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mread_csv(\u001b[39m\"\u001b[39m\u001b[39mtask2/y_train.csv\u001b[39m\u001b[39m\"\u001b[39m)\u001b[39m.\u001b[39mdrop(\u001b[39m'\u001b[39m\u001b[39mid\u001b[39m\u001b[39m'\u001b[39m, axis\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\zheng\\anaconda3\\Lib\\site-packages\\pandas\\core\\frame.py:758\u001b[0m, in \u001b[0;36mDataFrame.__init__\u001b[1;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[0;32m    747\u001b[0m         mgr \u001b[39m=\u001b[39m dict_to_mgr(\n\u001b[0;32m    748\u001b[0m             \u001b[39m# error: Item \"ndarray\" of \"Union[ndarray, Series, Index]\" has no\u001b[39;00m\n\u001b[0;32m    749\u001b[0m             \u001b[39m# attribute \"name\"\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    755\u001b[0m             copy\u001b[39m=\u001b[39m_copy,\n\u001b[0;32m    756\u001b[0m         )\n\u001b[0;32m    757\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 758\u001b[0m         mgr \u001b[39m=\u001b[39m ndarray_to_mgr(\n\u001b[0;32m    759\u001b[0m             data,\n\u001b[0;32m    760\u001b[0m             index,\n\u001b[0;32m    761\u001b[0m             columns,\n\u001b[0;32m    762\u001b[0m             dtype\u001b[39m=\u001b[39mdtype,\n\u001b[0;32m    763\u001b[0m             copy\u001b[39m=\u001b[39mcopy,\n\u001b[0;32m    764\u001b[0m             typ\u001b[39m=\u001b[39mmanager,\n\u001b[0;32m    765\u001b[0m         )\n\u001b[0;32m    767\u001b[0m \u001b[39m# For data is list-like, or Iterable (will consume into list)\u001b[39;00m\n\u001b[0;32m    768\u001b[0m \u001b[39melif\u001b[39;00m is_list_like(data):\n",
      "File \u001b[1;32mc:\\Users\\zheng\\anaconda3\\Lib\\site-packages\\pandas\\core\\internals\\construction.py:337\u001b[0m, in \u001b[0;36mndarray_to_mgr\u001b[1;34m(values, index, columns, dtype, copy, typ)\u001b[0m\n\u001b[0;32m    332\u001b[0m \u001b[39m# _prep_ndarraylike ensures that values.ndim == 2 at this point\u001b[39;00m\n\u001b[0;32m    333\u001b[0m index, columns \u001b[39m=\u001b[39m _get_axes(\n\u001b[0;32m    334\u001b[0m     values\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m], values\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m], index\u001b[39m=\u001b[39mindex, columns\u001b[39m=\u001b[39mcolumns\n\u001b[0;32m    335\u001b[0m )\n\u001b[1;32m--> 337\u001b[0m _check_values_indices_shape_match(values, index, columns)\n\u001b[0;32m    339\u001b[0m \u001b[39mif\u001b[39;00m typ \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39marray\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m    340\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39missubclass\u001b[39m(values\u001b[39m.\u001b[39mdtype\u001b[39m.\u001b[39mtype, \u001b[39mstr\u001b[39m):\n",
      "File \u001b[1;32mc:\\Users\\zheng\\anaconda3\\Lib\\site-packages\\pandas\\core\\internals\\construction.py:408\u001b[0m, in \u001b[0;36m_check_values_indices_shape_match\u001b[1;34m(values, index, columns)\u001b[0m\n\u001b[0;32m    406\u001b[0m passed \u001b[39m=\u001b[39m values\u001b[39m.\u001b[39mshape\n\u001b[0;32m    407\u001b[0m implied \u001b[39m=\u001b[39m (\u001b[39mlen\u001b[39m(index), \u001b[39mlen\u001b[39m(columns))\n\u001b[1;32m--> 408\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mShape of passed values is \u001b[39m\u001b[39m{\u001b[39;00mpassed\u001b[39m}\u001b[39;00m\u001b[39m, indices imply \u001b[39m\u001b[39m{\u001b[39;00mimplied\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "\u001b[1;31mValueError\u001b[0m: Shape of passed values is (3411, 330), indices imply (3411, 333)"
     ]
    }
   ],
   "source": [
    "X_train_features.columns = X_train_features.columns.astype(str)\n",
    "X_test_features.columns = X_test_features.columns.astype(str)\n",
    "\n",
    "\n",
    "imputer = SimpleImputer(strategy='median')\n",
    "X_train_features = pd.DataFrame(imputer.fit_transform(X_train_features), columns=X_train_features.columns)\n",
    "X_test_features = pd.DataFrame(imputer.fit_transform(X_test_features), columns=X_train_features.columns)\n",
    "y_train = pd.read_csv(\"task2/y_train.csv\").drop('id', axis=1)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\zheng\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\validation.py:1184: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "c:\\Users\\zheng\\anaconda3\\Lib\\site-packages\\sklearn\\feature_selection\\_univariate_selection.py:112: UserWarning: Features [1 2] are constant.\n",
      "  warnings.warn(\"Features %s are constant.\" % constant_features_idx, UserWarning)\n",
      "c:\\Users\\zheng\\anaconda3\\Lib\\site-packages\\sklearn\\feature_selection\\_univariate_selection.py:113: RuntimeWarning: invalid value encountered in divide\n",
      "  f = msb / msw\n",
      "c:\\Users\\zheng\\anaconda3\\Lib\\site-packages\\sklearn\\ensemble\\_gb.py:424: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training\n",
      "Predicting\n",
      "Scoring\n",
      ">0.813\n",
      "Scoring\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[557,   0,  37,   1],\n",
       "       [  5,  61,  18,   2],\n",
       "       [ 92,  12, 194,   4],\n",
       "       [ 13,   1,   6,  21]], dtype=int64)"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "select_k_best = SelectKBest(f_classif, k=300)  # Adjust 'k' as needed\n",
    "X_new = select_k_best.fit_transform(X_train_features, y_train)\n",
    "x_train, x_test, y_train_, y_test_ = train_test_split(X_new, y_train, test_size=0.20, random_state=42)\n",
    "\n",
    "clf = GradientBoostingClassifier(learning_rate=0.05, n_estimators=500, max_depth=7, \n",
    "                                 min_samples_split=60, min_samples_leaf=9, subsample=1.0,\n",
    "                                 max_features=50, random_state=0)\n",
    "\n",
    "\n",
    "print ('Training')\n",
    "eval_set = [(x_test, y_test_)]\n",
    "clf.fit(x_train, y_train_)\n",
    "    \n",
    "print ('Predicting')\n",
    "predicted_labels = clf.predict(x_test)\n",
    "\n",
    "print ('Scoring')\n",
    "score = f1_score(y_test_, predicted_labels, average='micro')\n",
    "\n",
    "print('>%.3f' % score)\n",
    "\n",
    "print ('Scoring')\n",
    "confusion_matrix(y_test_, predicted_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\zheng\\anaconda3\\Lib\\site-packages\\sklearn\\ensemble\\_gb.py:424: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\zheng\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:457: UserWarning: X has feature names, but GradientBoostingClassifier was fitted without feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "X has 333 features, but GradientBoostingClassifier is expecting 300 features as input.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\zheng\\Desktop\\ETH Third Semester\\AML\\AML projects\\Second\\ViktorExploration.ipynb Cell 11\u001b[0m line \u001b[0;36m8\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/zheng/Desktop/ETH%20Third%20Semester/AML/AML%20projects/Second/ViktorExploration.ipynb#X53sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m clf\u001b[39m.\u001b[39mfit(X_new, y_train)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/zheng/Desktop/ETH%20Third%20Semester/AML/AML%20projects/Second/ViktorExploration.ipynb#X53sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m \u001b[39mprint\u001b[39m (\u001b[39m'\u001b[39m\u001b[39mPredicting\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/zheng/Desktop/ETH%20Third%20Semester/AML/AML%20projects/Second/ViktorExploration.ipynb#X53sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m predicted_labels \u001b[39m=\u001b[39m clf\u001b[39m.\u001b[39mpredict(X_test_features)\n",
      "File \u001b[1;32mc:\\Users\\zheng\\anaconda3\\Lib\\site-packages\\sklearn\\ensemble\\_gb.py:1286\u001b[0m, in \u001b[0;36mGradientBoostingClassifier.predict\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m   1271\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mpredict\u001b[39m(\u001b[39mself\u001b[39m, X):\n\u001b[0;32m   1272\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Predict class for X.\u001b[39;00m\n\u001b[0;32m   1273\u001b[0m \n\u001b[0;32m   1274\u001b[0m \u001b[39m    Parameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1284\u001b[0m \u001b[39m        The predicted values.\u001b[39;00m\n\u001b[0;32m   1285\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1286\u001b[0m     raw_predictions \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdecision_function(X)\n\u001b[0;32m   1287\u001b[0m     encoded_labels \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_loss\u001b[39m.\u001b[39m_raw_prediction_to_decision(raw_predictions)\n\u001b[0;32m   1288\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclasses_\u001b[39m.\u001b[39mtake(encoded_labels, axis\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\zheng\\anaconda3\\Lib\\site-packages\\sklearn\\ensemble\\_gb.py:1239\u001b[0m, in \u001b[0;36mGradientBoostingClassifier.decision_function\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m   1220\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecision_function\u001b[39m(\u001b[39mself\u001b[39m, X):\n\u001b[0;32m   1221\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Compute the decision function of ``X``.\u001b[39;00m\n\u001b[0;32m   1222\u001b[0m \n\u001b[0;32m   1223\u001b[0m \u001b[39m    Parameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1237\u001b[0m \u001b[39m        array of shape (n_samples,).\u001b[39;00m\n\u001b[0;32m   1238\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1239\u001b[0m     X \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_validate_data(\n\u001b[0;32m   1240\u001b[0m         X, dtype\u001b[39m=\u001b[39mDTYPE, order\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mC\u001b[39m\u001b[39m\"\u001b[39m, accept_sparse\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mcsr\u001b[39m\u001b[39m\"\u001b[39m, reset\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m\n\u001b[0;32m   1241\u001b[0m     )\n\u001b[0;32m   1242\u001b[0m     raw_predictions \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_raw_predict(X)\n\u001b[0;32m   1243\u001b[0m     \u001b[39mif\u001b[39;00m raw_predictions\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m] \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\zheng\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:625\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[1;34m(self, X, y, reset, validate_separately, cast_to_ndarray, **check_params)\u001b[0m\n\u001b[0;32m    622\u001b[0m     out \u001b[39m=\u001b[39m X, y\n\u001b[0;32m    624\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m no_val_X \u001b[39mand\u001b[39;00m check_params\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mensure_2d\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mTrue\u001b[39;00m):\n\u001b[1;32m--> 625\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_n_features(X, reset\u001b[39m=\u001b[39mreset)\n\u001b[0;32m    627\u001b[0m \u001b[39mreturn\u001b[39;00m out\n",
      "File \u001b[1;32mc:\\Users\\zheng\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:414\u001b[0m, in \u001b[0;36mBaseEstimator._check_n_features\u001b[1;34m(self, X, reset)\u001b[0m\n\u001b[0;32m    411\u001b[0m     \u001b[39mreturn\u001b[39;00m\n\u001b[0;32m    413\u001b[0m \u001b[39mif\u001b[39;00m n_features \u001b[39m!=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_features_in_:\n\u001b[1;32m--> 414\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    415\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mX has \u001b[39m\u001b[39m{\u001b[39;00mn_features\u001b[39m}\u001b[39;00m\u001b[39m features, but \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    416\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mis expecting \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_features_in_\u001b[39m}\u001b[39;00m\u001b[39m features as input.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    417\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: X has 333 features, but GradientBoostingClassifier is expecting 300 features as input."
     ]
    }
   ],
   "source": [
    "X_test_new = select_k_best.transform(X_test_features)\n",
    "clf = GradientBoostingClassifier(learning_rate=0.05, n_estimators=500, max_depth=7, \n",
    "                                 min_samples_split=60, min_samples_leaf=9, subsample=1.0,\n",
    "                                 max_features=50, random_state=0)\n",
    "clf.fit(X_new, y_train)\n",
    "\n",
    "\n",
    "predicted_labels = clf.predict(X_test_new)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Input X contains NaN.\nSelectKBest does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\zheng\\Desktop\\ETH Third Semester\\AML\\AML projects\\Second\\ViktorExploration.ipynb Cell 13\u001b[0m line \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/zheng/Desktop/ETH%20Third%20Semester/AML/AML%20projects/Second/ViktorExploration.ipynb#X54sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m X_test_new \u001b[39m=\u001b[39m select_k_best\u001b[39m.\u001b[39mtransform(X_test_features)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/zheng/Desktop/ETH%20Third%20Semester/AML/AML%20projects/Second/ViktorExploration.ipynb#X54sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m predicted_labels \u001b[39m=\u001b[39m clf\u001b[39m.\u001b[39mpredict(X_test_new)\n",
      "File \u001b[1;32mc:\\Users\\zheng\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\_set_output.py:140\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[1;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[0;32m    138\u001b[0m \u001b[39m@wraps\u001b[39m(f)\n\u001b[0;32m    139\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mwrapped\u001b[39m(\u001b[39mself\u001b[39m, X, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m--> 140\u001b[0m     data_to_wrap \u001b[39m=\u001b[39m f(\u001b[39mself\u001b[39m, X, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    141\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(data_to_wrap, \u001b[39mtuple\u001b[39m):\n\u001b[0;32m    142\u001b[0m         \u001b[39m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[0;32m    143\u001b[0m         return_tuple \u001b[39m=\u001b[39m (\n\u001b[0;32m    144\u001b[0m             _wrap_data_with_container(method, data_to_wrap[\u001b[39m0\u001b[39m], X, \u001b[39mself\u001b[39m),\n\u001b[0;32m    145\u001b[0m             \u001b[39m*\u001b[39mdata_to_wrap[\u001b[39m1\u001b[39m:],\n\u001b[0;32m    146\u001b[0m         )\n",
      "File \u001b[1;32mc:\\Users\\zheng\\anaconda3\\Lib\\site-packages\\sklearn\\feature_selection\\_base.py:88\u001b[0m, in \u001b[0;36mSelectorMixin.transform\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m     84\u001b[0m preserve_X \u001b[39m=\u001b[39m \u001b[39mhasattr\u001b[39m(X, \u001b[39m\"\u001b[39m\u001b[39miloc\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mand\u001b[39;00m output_config_dense \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mpandas\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m     86\u001b[0m \u001b[39m# note: we use _safe_tags instead of _get_tags because this is a\u001b[39;00m\n\u001b[0;32m     87\u001b[0m \u001b[39m# public Mixin.\u001b[39;00m\n\u001b[1;32m---> 88\u001b[0m X \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_validate_data(\n\u001b[0;32m     89\u001b[0m     X,\n\u001b[0;32m     90\u001b[0m     dtype\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m,\n\u001b[0;32m     91\u001b[0m     accept_sparse\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mcsr\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m     92\u001b[0m     force_all_finite\u001b[39m=\u001b[39m\u001b[39mnot\u001b[39;00m _safe_tags(\u001b[39mself\u001b[39m, key\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mallow_nan\u001b[39m\u001b[39m\"\u001b[39m),\n\u001b[0;32m     93\u001b[0m     cast_to_ndarray\u001b[39m=\u001b[39m\u001b[39mnot\u001b[39;00m preserve_X,\n\u001b[0;32m     94\u001b[0m     reset\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m,\n\u001b[0;32m     95\u001b[0m )\n\u001b[0;32m     96\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_transform(X)\n",
      "File \u001b[1;32mc:\\Users\\zheng\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:604\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[1;34m(self, X, y, reset, validate_separately, cast_to_ndarray, **check_params)\u001b[0m\n\u001b[0;32m    602\u001b[0m         out \u001b[39m=\u001b[39m X, y\n\u001b[0;32m    603\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mnot\u001b[39;00m no_val_X \u001b[39mand\u001b[39;00m no_val_y:\n\u001b[1;32m--> 604\u001b[0m     out \u001b[39m=\u001b[39m check_array(X, input_name\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mX\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mcheck_params)\n\u001b[0;32m    605\u001b[0m \u001b[39melif\u001b[39;00m no_val_X \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m no_val_y:\n\u001b[0;32m    606\u001b[0m     out \u001b[39m=\u001b[39m _check_y(y, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mcheck_params)\n",
      "File \u001b[1;32mc:\\Users\\zheng\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\validation.py:959\u001b[0m, in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[0;32m    953\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    954\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mFound array with dim \u001b[39m\u001b[39m%d\u001b[39;00m\u001b[39m. \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m expected <= 2.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    955\u001b[0m             \u001b[39m%\u001b[39m (array\u001b[39m.\u001b[39mndim, estimator_name)\n\u001b[0;32m    956\u001b[0m         )\n\u001b[0;32m    958\u001b[0m     \u001b[39mif\u001b[39;00m force_all_finite:\n\u001b[1;32m--> 959\u001b[0m         _assert_all_finite(\n\u001b[0;32m    960\u001b[0m             array,\n\u001b[0;32m    961\u001b[0m             input_name\u001b[39m=\u001b[39minput_name,\n\u001b[0;32m    962\u001b[0m             estimator_name\u001b[39m=\u001b[39mestimator_name,\n\u001b[0;32m    963\u001b[0m             allow_nan\u001b[39m=\u001b[39mforce_all_finite \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mallow-nan\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m    964\u001b[0m         )\n\u001b[0;32m    966\u001b[0m \u001b[39mif\u001b[39;00m ensure_min_samples \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m    967\u001b[0m     n_samples \u001b[39m=\u001b[39m _num_samples(array)\n",
      "File \u001b[1;32mc:\\Users\\zheng\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\validation.py:124\u001b[0m, in \u001b[0;36m_assert_all_finite\u001b[1;34m(X, allow_nan, msg_dtype, estimator_name, input_name)\u001b[0m\n\u001b[0;32m    121\u001b[0m \u001b[39mif\u001b[39;00m first_pass_isfinite:\n\u001b[0;32m    122\u001b[0m     \u001b[39mreturn\u001b[39;00m\n\u001b[1;32m--> 124\u001b[0m _assert_all_finite_element_wise(\n\u001b[0;32m    125\u001b[0m     X,\n\u001b[0;32m    126\u001b[0m     xp\u001b[39m=\u001b[39mxp,\n\u001b[0;32m    127\u001b[0m     allow_nan\u001b[39m=\u001b[39mallow_nan,\n\u001b[0;32m    128\u001b[0m     msg_dtype\u001b[39m=\u001b[39mmsg_dtype,\n\u001b[0;32m    129\u001b[0m     estimator_name\u001b[39m=\u001b[39mestimator_name,\n\u001b[0;32m    130\u001b[0m     input_name\u001b[39m=\u001b[39minput_name,\n\u001b[0;32m    131\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\zheng\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\validation.py:173\u001b[0m, in \u001b[0;36m_assert_all_finite_element_wise\u001b[1;34m(X, xp, allow_nan, msg_dtype, estimator_name, input_name)\u001b[0m\n\u001b[0;32m    156\u001b[0m \u001b[39mif\u001b[39;00m estimator_name \u001b[39mand\u001b[39;00m input_name \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mX\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mand\u001b[39;00m has_nan_error:\n\u001b[0;32m    157\u001b[0m     \u001b[39m# Improve the error message on how to handle missing values in\u001b[39;00m\n\u001b[0;32m    158\u001b[0m     \u001b[39m# scikit-learn.\u001b[39;00m\n\u001b[0;32m    159\u001b[0m     msg_err \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m (\n\u001b[0;32m    160\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00mestimator_name\u001b[39m}\u001b[39;00m\u001b[39m does not accept missing values\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    161\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m encoded as NaN natively. For supervised learning, you might want\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    171\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m#estimators-that-handle-nan-values\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    172\u001b[0m     )\n\u001b[1;32m--> 173\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(msg_err)\n",
      "\u001b[1;31mValueError\u001b[0m: Input X contains NaN.\nSelectKBest does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values"
     ]
    }
   ],
   "source": [
    "X_test_new = select_k_best.transform(X_test_features)\n",
    "predicted_labels = clf.predict(X_test_new)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5117, 300)"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_new.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(\"FirstMLModel.csv\", \"w\")\n",
    "f.write(\"id,y\\n\")\n",
    "for i,x in enumerate(predicted_labels):\n",
    "    f.write(\"{},{}\\n\".format(i,predicted_labels[i]))\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5117"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(predicted_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5117, 300)"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\zheng\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\validation.py:1184: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "c:\\Users\\zheng\\anaconda3\\Lib\\site-packages\\sklearn\\feature_selection\\_univariate_selection.py:112: UserWarning: Features [  1   2 946] are constant.\n",
      "  warnings.warn(\"Features %s are constant.\" % constant_features_idx, UserWarning)\n",
      "c:\\Users\\zheng\\anaconda3\\Lib\\site-packages\\sklearn\\feature_selection\\_univariate_selection.py:113: RuntimeWarning: invalid value encountered in divide\n",
      "  f = msb / msw\n",
      "c:\\Users\\zheng\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:1151: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "c:\\Users\\zheng\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:1151: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "c:\\Users\\zheng\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:1151: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "c:\\Users\\zheng\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:1151: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "c:\\Users\\zheng\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:1151: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "c:\\Users\\zheng\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:1151: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "c:\\Users\\zheng\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:1151: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "c:\\Users\\zheng\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:1151: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "c:\\Users\\zheng\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:1151: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "c:\\Users\\zheng\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:1151: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "c:\\Users\\zheng\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:1151: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "c:\\Users\\zheng\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:1151: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "c:\\Users\\zheng\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:1151: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "c:\\Users\\zheng\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:1151: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "c:\\Users\\zheng\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:1151: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "c:\\Users\\zheng\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:1151: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "c:\\Users\\zheng\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:1151: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "c:\\Users\\zheng\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:1151: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "c:\\Users\\zheng\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:1151: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "c:\\Users\\zheng\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:1151: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "c:\\Users\\zheng\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:1151: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters for RandomForest: {'max_depth': 10, 'n_estimators': 100}\n",
      "F1 Score for RandomForest: 0.6984935593305222\n",
      "Classification Report for RandomForest:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.73      0.94      0.82       733\n",
      "           1       0.73      0.32      0.44       111\n",
      "           2       0.71      0.48      0.57       385\n",
      "           3       0.70      0.31      0.43        51\n",
      "\n",
      "    accuracy                           0.72      1280\n",
      "   macro avg       0.72      0.51      0.57      1280\n",
      "weighted avg       0.72      0.72      0.70      1280\n",
      "\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006254 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 49835\n",
      "[LightGBM] [Info] Number of data points in the train set: 3069, number of used features: 200\n",
      "[LightGBM] [Info] Start training from score -0.513218\n",
      "[LightGBM] [Info] Start training from score -2.445611\n",
      "[LightGBM] [Info] Start training from score -1.259465\n",
      "[LightGBM] [Info] Start training from score -3.475230\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\zheng\\anaconda3\\Lib\\site-packages\\sklearn\\preprocessing\\_label.py:97: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "c:\\Users\\zheng\\anaconda3\\Lib\\site-packages\\sklearn\\preprocessing\\_label.py:132: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, dtype=self.classes_.dtype, warn=True)\n",
      "c:\\Users\\zheng\\anaconda3\\Lib\\site-packages\\sklearn\\preprocessing\\_label.py:97: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "c:\\Users\\zheng\\anaconda3\\Lib\\site-packages\\sklearn\\preprocessing\\_label.py:132: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, dtype=self.classes_.dtype, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.007002 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 49814\n",
      "[LightGBM] [Info] Number of data points in the train set: 3069, number of used features: 200\n",
      "[LightGBM] [Info] Start training from score -0.513218\n",
      "[LightGBM] [Info] Start training from score -2.445611\n",
      "[LightGBM] [Info] Start training from score -1.259465\n",
      "[LightGBM] [Info] Start training from score -3.475230\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\zheng\\anaconda3\\Lib\\site-packages\\sklearn\\preprocessing\\_label.py:97: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "c:\\Users\\zheng\\anaconda3\\Lib\\site-packages\\sklearn\\preprocessing\\_label.py:132: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, dtype=self.classes_.dtype, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.007583 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 49835\n",
      "[LightGBM] [Info] Number of data points in the train set: 3070, number of used features: 200\n",
      "[LightGBM] [Info] Start training from score -0.513000\n",
      "[LightGBM] [Info] Start training from score -2.449703\n",
      "[LightGBM] [Info] Start training from score -1.259791\n",
      "[LightGBM] [Info] Start training from score -3.465085\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\zheng\\anaconda3\\Lib\\site-packages\\sklearn\\preprocessing\\_label.py:97: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "c:\\Users\\zheng\\anaconda3\\Lib\\site-packages\\sklearn\\preprocessing\\_label.py:132: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, dtype=self.classes_.dtype, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006997 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 49827\n",
      "[LightGBM] [Info] Number of data points in the train set: 3070, number of used features: 200\n",
      "[LightGBM] [Info] Start training from score -0.513000\n",
      "[LightGBM] [Info] Start training from score -2.449703\n",
      "[LightGBM] [Info] Start training from score -1.258643\n",
      "[LightGBM] [Info] Start training from score -3.475556\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\zheng\\anaconda3\\Lib\\site-packages\\sklearn\\preprocessing\\_label.py:97: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "c:\\Users\\zheng\\anaconda3\\Lib\\site-packages\\sklearn\\preprocessing\\_label.py:132: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, dtype=self.classes_.dtype, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006817 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 49817\n",
      "[LightGBM] [Info] Number of data points in the train set: 3070, number of used features: 200\n",
      "[LightGBM] [Info] Start training from score -0.513000\n",
      "[LightGBM] [Info] Start training from score -2.445937\n",
      "[LightGBM] [Info] Start training from score -1.259791\n",
      "[LightGBM] [Info] Start training from score -3.475556\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\zheng\\anaconda3\\Lib\\site-packages\\sklearn\\preprocessing\\_label.py:97: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "c:\\Users\\zheng\\anaconda3\\Lib\\site-packages\\sklearn\\preprocessing\\_label.py:132: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, dtype=self.classes_.dtype, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.007285 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 49835\n",
      "[LightGBM] [Info] Number of data points in the train set: 3069, number of used features: 200\n",
      "[LightGBM] [Info] Start training from score -0.513218\n",
      "[LightGBM] [Info] Start training from score -2.445611\n",
      "[LightGBM] [Info] Start training from score -1.259465\n",
      "[LightGBM] [Info] Start training from score -3.475230\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\zheng\\anaconda3\\Lib\\site-packages\\sklearn\\preprocessing\\_label.py:97: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "c:\\Users\\zheng\\anaconda3\\Lib\\site-packages\\sklearn\\preprocessing\\_label.py:132: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, dtype=self.classes_.dtype, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.007954 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 49814\n",
      "[LightGBM] [Info] Number of data points in the train set: 3069, number of used features: 200\n",
      "[LightGBM] [Info] Start training from score -0.513218\n",
      "[LightGBM] [Info] Start training from score -2.445611\n",
      "[LightGBM] [Info] Start training from score -1.259465\n",
      "[LightGBM] [Info] Start training from score -3.475230\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\zheng\\anaconda3\\Lib\\site-packages\\sklearn\\preprocessing\\_label.py:97: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "c:\\Users\\zheng\\anaconda3\\Lib\\site-packages\\sklearn\\preprocessing\\_label.py:132: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, dtype=self.classes_.dtype, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.007769 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 49835\n",
      "[LightGBM] [Info] Number of data points in the train set: 3070, number of used features: 200\n",
      "[LightGBM] [Info] Start training from score -0.513000\n",
      "[LightGBM] [Info] Start training from score -2.449703\n",
      "[LightGBM] [Info] Start training from score -1.259791\n",
      "[LightGBM] [Info] Start training from score -3.465085\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\zheng\\anaconda3\\Lib\\site-packages\\sklearn\\preprocessing\\_label.py:97: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "c:\\Users\\zheng\\anaconda3\\Lib\\site-packages\\sklearn\\preprocessing\\_label.py:132: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, dtype=self.classes_.dtype, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.007098 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 49827\n",
      "[LightGBM] [Info] Number of data points in the train set: 3070, number of used features: 200\n",
      "[LightGBM] [Info] Start training from score -0.513000\n",
      "[LightGBM] [Info] Start training from score -2.449703\n",
      "[LightGBM] [Info] Start training from score -1.258643\n",
      "[LightGBM] [Info] Start training from score -3.475556\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\zheng\\anaconda3\\Lib\\site-packages\\sklearn\\preprocessing\\_label.py:97: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "c:\\Users\\zheng\\anaconda3\\Lib\\site-packages\\sklearn\\preprocessing\\_label.py:132: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, dtype=self.classes_.dtype, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006972 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 49817\n",
      "[LightGBM] [Info] Number of data points in the train set: 3070, number of used features: 200\n",
      "[LightGBM] [Info] Start training from score -0.513000\n",
      "[LightGBM] [Info] Start training from score -2.445937\n",
      "[LightGBM] [Info] Start training from score -1.259791\n",
      "[LightGBM] [Info] Start training from score -3.475556\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\zheng\\anaconda3\\Lib\\site-packages\\sklearn\\preprocessing\\_label.py:97: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "c:\\Users\\zheng\\anaconda3\\Lib\\site-packages\\sklearn\\preprocessing\\_label.py:132: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, dtype=self.classes_.dtype, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.007521 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 49835\n",
      "[LightGBM] [Info] Number of data points in the train set: 3069, number of used features: 200\n",
      "[LightGBM] [Info] Start training from score -0.513218\n",
      "[LightGBM] [Info] Start training from score -2.445611\n",
      "[LightGBM] [Info] Start training from score -1.259465\n",
      "[LightGBM] [Info] Start training from score -3.475230\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\zheng\\anaconda3\\Lib\\site-packages\\sklearn\\preprocessing\\_label.py:97: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "c:\\Users\\zheng\\anaconda3\\Lib\\site-packages\\sklearn\\preprocessing\\_label.py:132: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, dtype=self.classes_.dtype, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.007302 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 49814\n",
      "[LightGBM] [Info] Number of data points in the train set: 3069, number of used features: 200\n",
      "[LightGBM] [Info] Start training from score -0.513218\n",
      "[LightGBM] [Info] Start training from score -2.445611\n",
      "[LightGBM] [Info] Start training from score -1.259465\n",
      "[LightGBM] [Info] Start training from score -3.475230\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\zheng\\anaconda3\\Lib\\site-packages\\sklearn\\preprocessing\\_label.py:97: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "c:\\Users\\zheng\\anaconda3\\Lib\\site-packages\\sklearn\\preprocessing\\_label.py:132: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, dtype=self.classes_.dtype, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.007619 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 49835\n",
      "[LightGBM] [Info] Number of data points in the train set: 3070, number of used features: 200\n",
      "[LightGBM] [Info] Start training from score -0.513000\n",
      "[LightGBM] [Info] Start training from score -2.449703\n",
      "[LightGBM] [Info] Start training from score -1.259791\n",
      "[LightGBM] [Info] Start training from score -3.465085\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\zheng\\anaconda3\\Lib\\site-packages\\sklearn\\preprocessing\\_label.py:97: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "c:\\Users\\zheng\\anaconda3\\Lib\\site-packages\\sklearn\\preprocessing\\_label.py:132: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, dtype=self.classes_.dtype, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.007269 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 49827\n",
      "[LightGBM] [Info] Number of data points in the train set: 3070, number of used features: 200\n",
      "[LightGBM] [Info] Start training from score -0.513000\n",
      "[LightGBM] [Info] Start training from score -2.449703\n",
      "[LightGBM] [Info] Start training from score -1.258643\n",
      "[LightGBM] [Info] Start training from score -3.475556\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\zheng\\anaconda3\\Lib\\site-packages\\sklearn\\preprocessing\\_label.py:97: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "c:\\Users\\zheng\\anaconda3\\Lib\\site-packages\\sklearn\\preprocessing\\_label.py:132: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, dtype=self.classes_.dtype, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.007064 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 49817\n",
      "[LightGBM] [Info] Number of data points in the train set: 3070, number of used features: 200\n",
      "[LightGBM] [Info] Start training from score -0.513000\n",
      "[LightGBM] [Info] Start training from score -2.445937\n",
      "[LightGBM] [Info] Start training from score -1.259791\n",
      "[LightGBM] [Info] Start training from score -3.475556\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\zheng\\anaconda3\\Lib\\site-packages\\sklearn\\preprocessing\\_label.py:97: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "c:\\Users\\zheng\\anaconda3\\Lib\\site-packages\\sklearn\\preprocessing\\_label.py:132: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, dtype=self.classes_.dtype, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.008428 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 49835\n",
      "[LightGBM] [Info] Number of data points in the train set: 3069, number of used features: 200\n",
      "[LightGBM] [Info] Start training from score -0.513218\n",
      "[LightGBM] [Info] Start training from score -2.445611\n",
      "[LightGBM] [Info] Start training from score -1.259465\n",
      "[LightGBM] [Info] Start training from score -3.475230\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\zheng\\anaconda3\\Lib\\site-packages\\sklearn\\preprocessing\\_label.py:97: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "c:\\Users\\zheng\\anaconda3\\Lib\\site-packages\\sklearn\\preprocessing\\_label.py:132: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, dtype=self.classes_.dtype, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006987 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 49814\n",
      "[LightGBM] [Info] Number of data points in the train set: 3069, number of used features: 200\n",
      "[LightGBM] [Info] Start training from score -0.513218\n",
      "[LightGBM] [Info] Start training from score -2.445611\n",
      "[LightGBM] [Info] Start training from score -1.259465\n",
      "[LightGBM] [Info] Start training from score -3.475230\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\zheng\\anaconda3\\Lib\\site-packages\\sklearn\\preprocessing\\_label.py:97: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "c:\\Users\\zheng\\anaconda3\\Lib\\site-packages\\sklearn\\preprocessing\\_label.py:132: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, dtype=self.classes_.dtype, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.007926 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 49835\n",
      "[LightGBM] [Info] Number of data points in the train set: 3070, number of used features: 200\n",
      "[LightGBM] [Info] Start training from score -0.513000\n",
      "[LightGBM] [Info] Start training from score -2.449703\n",
      "[LightGBM] [Info] Start training from score -1.259791\n",
      "[LightGBM] [Info] Start training from score -3.465085\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\zheng\\anaconda3\\Lib\\site-packages\\sklearn\\preprocessing\\_label.py:97: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "c:\\Users\\zheng\\anaconda3\\Lib\\site-packages\\sklearn\\preprocessing\\_label.py:132: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, dtype=self.classes_.dtype, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.007535 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 49827\n",
      "[LightGBM] [Info] Number of data points in the train set: 3070, number of used features: 200\n",
      "[LightGBM] [Info] Start training from score -0.513000\n",
      "[LightGBM] [Info] Start training from score -2.449703\n",
      "[LightGBM] [Info] Start training from score -1.258643\n",
      "[LightGBM] [Info] Start training from score -3.475556\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\zheng\\anaconda3\\Lib\\site-packages\\sklearn\\preprocessing\\_label.py:97: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "c:\\Users\\zheng\\anaconda3\\Lib\\site-packages\\sklearn\\preprocessing\\_label.py:132: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, dtype=self.classes_.dtype, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.007258 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 49817\n",
      "[LightGBM] [Info] Number of data points in the train set: 3070, number of used features: 200\n",
      "[LightGBM] [Info] Start training from score -0.513000\n",
      "[LightGBM] [Info] Start training from score -2.445937\n",
      "[LightGBM] [Info] Start training from score -1.259791\n",
      "[LightGBM] [Info] Start training from score -3.475556\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\zheng\\anaconda3\\Lib\\site-packages\\sklearn\\preprocessing\\_label.py:97: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "c:\\Users\\zheng\\anaconda3\\Lib\\site-packages\\sklearn\\preprocessing\\_label.py:132: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, dtype=self.classes_.dtype, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.008028 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 49834\n",
      "[LightGBM] [Info] Number of data points in the train set: 3837, number of used features: 200\n",
      "[LightGBM] [Info] Start training from score -0.513087\n",
      "[LightGBM] [Info] Start training from score -2.447311\n",
      "[LightGBM] [Info] Start training from score -1.259431\n",
      "[LightGBM] [Info] Start training from score -3.473323\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "Best parameters for LightGBM: {'learning_rate': 0.1, 'n_estimators': 100}\n",
      "F1 Score for LightGBM: 0.7670893681048637\n",
      "Classification Report for LightGBM:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.94      0.86       733\n",
      "           1       0.77      0.50      0.60       111\n",
      "           2       0.75      0.58      0.65       385\n",
      "           3       0.65      0.55      0.60        51\n",
      "\n",
      "    accuracy                           0.78      1280\n",
      "   macro avg       0.74      0.64      0.68      1280\n",
      "weighted avg       0.77      0.78      0.77      1280\n",
      "\n",
      "Best parameters for XGBoost: {'learning_rate': 0.1, 'n_estimators': 100}\n",
      "F1 Score for XGBoost: 0.7569412794752475\n",
      "Classification Report for XGBoost:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.94      0.86       733\n",
      "           1       0.76      0.49      0.59       111\n",
      "           2       0.72      0.56      0.63       385\n",
      "           3       0.65      0.59      0.62        51\n",
      "\n",
      "    accuracy                           0.77      1280\n",
      "   macro avg       0.73      0.64      0.67      1280\n",
      "weighted avg       0.76      0.77      0.76      1280\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "y_train = pd.read_csv(\"task2/y_train.csv\").drop('id', axis=1)\n",
    "\n",
    "# Apply SelectKBest to extract top features\n",
    "select_k_best = SelectKBest(f_classif, k=200)  # Adjust 'k' as needed\n",
    "X_new = select_k_best.fit_transform(concatenated_features, y_train)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_new, y_train, test_size=0.25, random_state=1)\n",
    "\n",
    "models_and_parameters = {\n",
    "    'RandomForest': (RandomForestClassifier(), \n",
    "                     {'n_estimators': [50, 100], 'max_depth': [5, 10]}),\n",
    "    'LightGBM': (lgb.LGBMClassifier(), \n",
    "                 {'learning_rate': [0.01, 0.1], 'n_estimators': [50, 100]}),\n",
    "    'XGBoost': (xgb.XGBClassifier(use_label_encoder=False, eval_metric='logloss'), \n",
    "                {'learning_rate': [0.01, 0.1], 'n_estimators': [50, 100]})\n",
    "}\n",
    "\n",
    "# Define the F1 scorer\n",
    "f1_scorer = make_scorer(f1_score, average='weighted')\n",
    "\n",
    "# Iterate over models and perform grid search\n",
    "for model_name, (model, param_grid) in models_and_parameters.items():\n",
    "    clf = GridSearchCV(model, param_grid, cv=5, scoring=f1_scorer)\n",
    "    clf.fit(X_train, y_train)\n",
    "    print(f\"Best parameters for {model_name}: {clf.best_params_}\")\n",
    "    predictions = clf.predict(X_test)\n",
    "    print(f\"F1 Score for {model_name}: {f1_score(y_test, predictions, average='weighted')}\")\n",
    "    print(f\"Classification Report for {model_name}:\\n{classification_report(y_test, predictions)}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neorokit features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "NeuroKit error: ecg_intervalrelated(): Wrong input,we couldn't extract heart rate. Please make sureyour DataFrame contains an `ECG_Rate` column.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\zheng\\Desktop\\ETH Third Semester\\AML\\AML projects\\Second\\ViktorExploration.ipynb Cell 16\u001b[0m line \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/zheng/Desktop/ETH%20Third%20Semester/AML/AML%20projects/Second/ViktorExploration.ipynb#X21sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m neurokit_features \u001b[39m=\u001b[39m nk\u001b[39m.\u001b[39mecg_analyze(X_train, sampling_rate\u001b[39m=\u001b[39m\u001b[39m300\u001b[39m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/zheng/Desktop/ETH%20Third%20Semester/AML/AML%20projects/Second/ViktorExploration.ipynb#X21sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m neurokit_features\n",
      "File \u001b[1;32mc:\\Users\\zheng\\anaconda3\\Lib\\site-packages\\neurokit2\\ecg\\ecg_analyze.py:127\u001b[0m, in \u001b[0;36mecg_analyze\u001b[1;34m(data, sampling_rate, method)\u001b[0m\n\u001b[0;32m    125\u001b[0m     duration \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(data) \u001b[39m/\u001b[39m sampling_rate\n\u001b[0;32m    126\u001b[0m \u001b[39mif\u001b[39;00m duration \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m \u001b[39m10\u001b[39m:\n\u001b[1;32m--> 127\u001b[0m     features \u001b[39m=\u001b[39m ecg_intervalrelated(data, sampling_rate\u001b[39m=\u001b[39msampling_rate)\n\u001b[0;32m    128\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    129\u001b[0m     features \u001b[39m=\u001b[39m ecg_eventrelated(data)\n",
      "File \u001b[1;32mc:\\Users\\zheng\\anaconda3\\Lib\\site-packages\\neurokit2\\ecg\\ecg_intervalrelated.py:65\u001b[0m, in \u001b[0;36mecg_intervalrelated\u001b[1;34m(data, sampling_rate)\u001b[0m\n\u001b[0;32m     63\u001b[0m         intervals\u001b[39m.\u001b[39mupdate(_ecg_intervalrelated_hrv(data, sampling_rate))\n\u001b[0;32m     64\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m---> 65\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m     66\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mNeuroKit error: ecg_intervalrelated(): Wrong input,\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m     67\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mwe couldn\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt extract heart rate. Please make sure\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m     68\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39myour DataFrame contains an `ECG_Rate` column.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m     69\u001b[0m         )\n\u001b[0;32m     70\u001b[0m     ecg_intervals \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mDataFrame\u001b[39m.\u001b[39mfrom_dict(intervals, orient\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mindex\u001b[39m\u001b[39m\"\u001b[39m)\u001b[39m.\u001b[39mT\n\u001b[0;32m     72\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(data, \u001b[39mdict\u001b[39m):\n",
      "\u001b[1;31mValueError\u001b[0m: NeuroKit error: ecg_intervalrelated(): Wrong input,we couldn't extract heart rate. Please make sureyour DataFrame contains an `ECG_Rate` column."
     ]
    }
   ],
   "source": [
    "neurokit_features = nk.ecg_analyze(X_train, sampling_rate=300)\n",
    "neurokit_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\zheng\\anaconda3\\Lib\\site-packages\\neurokit2\\hrv\\hrv_nonlinear.py:474: NeuroKitWarning: DFA_alpha2 related indices will not be calculated. The maximum duration of the windows provided for the long-term correlation is smaller than the minimum duration of windows. Refer to the `scale` argument in `nk.fractal_dfa()` for more information.\n",
      "  warn(\n",
      "c:\\Users\\zheng\\anaconda3\\Lib\\site-packages\\neurokit2\\hrv\\hrv_nonlinear.py:474: NeuroKitWarning: DFA_alpha2 related indices will not be calculated. The maximum duration of the windows provided for the long-term correlation is smaller than the minimum duration of windows. Refer to the `scale` argument in `nk.fractal_dfa()` for more information.\n",
      "  warn(\n",
      "c:\\Users\\zheng\\anaconda3\\Lib\\site-packages\\neurokit2\\hrv\\hrv_nonlinear.py:474: NeuroKitWarning: DFA_alpha2 related indices will not be calculated. The maximum duration of the windows provided for the long-term correlation is smaller than the minimum duration of windows. Refer to the `scale` argument in `nk.fractal_dfa()` for more information.\n",
      "  warn(\n",
      "c:\\Users\\zheng\\anaconda3\\Lib\\site-packages\\neurokit2\\hrv\\hrv_nonlinear.py:474: NeuroKitWarning: DFA_alpha2 related indices will not be calculated. The maximum duration of the windows provided for the long-term correlation is smaller than the minimum duration of windows. Refer to the `scale` argument in `nk.fractal_dfa()` for more information.\n",
      "  warn(\n",
      "c:\\Users\\zheng\\anaconda3\\Lib\\site-packages\\neurokit2\\hrv\\hrv_nonlinear.py:474: NeuroKitWarning: DFA_alpha2 related indices will not be calculated. The maximum duration of the windows provided for the long-term correlation is smaller than the minimum duration of windows. Refer to the `scale` argument in `nk.fractal_dfa()` for more information.\n",
      "  warn(\n",
      "c:\\Users\\zheng\\anaconda3\\Lib\\site-packages\\neurokit2\\hrv\\hrv_nonlinear.py:474: NeuroKitWarning: DFA_alpha2 related indices will not be calculated. The maximum duration of the windows provided for the long-term correlation is smaller than the minimum duration of windows. Refer to the `scale` argument in `nk.fractal_dfa()` for more information.\n",
      "  warn(\n",
      "c:\\Users\\zheng\\anaconda3\\Lib\\site-packages\\neurokit2\\complexity\\entropy_multiscale.py:349: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  mse = np.trapz(mse) / len(mse)\n",
      "c:\\Users\\zheng\\anaconda3\\Lib\\site-packages\\neurokit2\\complexity\\entropy_multiscale.py:349: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  mse = np.trapz(mse) / len(mse)\n",
      "c:\\Users\\zheng\\anaconda3\\Lib\\site-packages\\neurokit2\\complexity\\entropy_multiscale.py:349: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  mse = np.trapz(mse) / len(mse)\n",
      "c:\\Users\\zheng\\anaconda3\\Lib\\site-packages\\neurokit2\\hrv\\hrv_nonlinear.py:474: NeuroKitWarning: DFA_alpha2 related indices will not be calculated. The maximum duration of the windows provided for the long-term correlation is smaller than the minimum duration of windows. Refer to the `scale` argument in `nk.fractal_dfa()` for more information.\n",
      "  warn(\n",
      "c:\\Users\\zheng\\anaconda3\\Lib\\site-packages\\neurokit2\\complexity\\entropy_multiscale.py:349: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  mse = np.trapz(mse) / len(mse)\n",
      "c:\\Users\\zheng\\anaconda3\\Lib\\site-packages\\neurokit2\\complexity\\entropy_multiscale.py:349: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  mse = np.trapz(mse) / len(mse)\n",
      "c:\\Users\\zheng\\anaconda3\\Lib\\site-packages\\neurokit2\\complexity\\entropy_multiscale.py:349: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  mse = np.trapz(mse) / len(mse)\n",
      "c:\\Users\\zheng\\anaconda3\\Lib\\site-packages\\neurokit2\\hrv\\hrv_nonlinear.py:474: NeuroKitWarning: DFA_alpha2 related indices will not be calculated. The maximum duration of the windows provided for the long-term correlation is smaller than the minimum duration of windows. Refer to the `scale` argument in `nk.fractal_dfa()` for more information.\n",
      "  warn(\n",
      "c:\\Users\\zheng\\anaconda3\\Lib\\site-packages\\neurokit2\\complexity\\entropy_multiscale.py:349: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  mse = np.trapz(mse) / len(mse)\n",
      "c:\\Users\\zheng\\anaconda3\\Lib\\site-packages\\neurokit2\\hrv\\hrv_nonlinear.py:474: NeuroKitWarning: DFA_alpha2 related indices will not be calculated. The maximum duration of the windows provided for the long-term correlation is smaller than the minimum duration of windows. Refer to the `scale` argument in `nk.fractal_dfa()` for more information.\n",
      "  warn(\n",
      "c:\\Users\\zheng\\anaconda3\\Lib\\site-packages\\neurokit2\\hrv\\hrv_nonlinear.py:474: NeuroKitWarning: DFA_alpha2 related indices will not be calculated. The maximum duration of the windows provided for the long-term correlation is smaller than the minimum duration of windows. Refer to the `scale` argument in `nk.fractal_dfa()` for more information.\n",
      "  warn(\n",
      "c:\\Users\\zheng\\anaconda3\\Lib\\site-packages\\neurokit2\\complexity\\entropy_multiscale.py:349: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  mse = np.trapz(mse) / len(mse)\n",
      "c:\\Users\\zheng\\anaconda3\\Lib\\site-packages\\neurokit2\\complexity\\optim_complexity_k.py:134: RuntimeWarning: divide by zero encountered in divide\n",
      "  normalization = (n - 1) / (np.floor((n - k_subrange) / k).astype(int) * k)\n",
      "c:\\Users\\zheng\\anaconda3\\Lib\\site-packages\\neurokit2\\complexity\\optim_complexity_k.py:135: RuntimeWarning: invalid value encountered in multiply\n",
      "  sets = (np.nansum(np.abs(np.diff(sig_values)), axis=1) * normalization) / k\n"
     ]
    },
    {
     "ename": "LinAlgError",
     "evalue": "SVD did not converge in Linear Least Squares",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mLinAlgError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\zheng\\Desktop\\ETH Third Semester\\AML\\AML projects\\Second\\ViktorExploration.ipynb Cell 18\u001b[0m line \u001b[0;36m1\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/zheng/Desktop/ETH%20Third%20Semester/AML/AML%20projects/Second/ViktorExploration.ipynb#X22sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m row \u001b[39m=\u001b[39m padded[i]\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/zheng/Desktop/ETH%20Third%20Semester/AML/AML%20projects/Second/ViktorExploration.ipynb#X22sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m \u001b[39m# Process each row and extract features\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/zheng/Desktop/ETH%20Third%20Semester/AML/AML%20projects/Second/ViktorExploration.ipynb#X22sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m features \u001b[39m=\u001b[39m process_ecg(row)    \n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/zheng/Desktop/ETH%20Third%20Semester/AML/AML%20projects/Second/ViktorExploration.ipynb#X22sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m \u001b[39m# Append the features to the features_df\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/zheng/Desktop/ETH%20Third%20Semester/AML/AML%20projects/Second/ViktorExploration.ipynb#X22sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m neurokit_features_df \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mconcat([neurokit_features_df, features], ignore_index\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n",
      "\u001b[1;32mc:\\Users\\zheng\\Desktop\\ETH Third Semester\\AML\\AML projects\\Second\\ViktorExploration.ipynb Cell 18\u001b[0m line \u001b[0;36m3\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/zheng/Desktop/ETH%20Third%20Semester/AML/AML%20projects/Second/ViktorExploration.ipynb#X22sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mprocess_ecg\u001b[39m(signal, sampling_rate\u001b[39m=\u001b[39m\u001b[39m300\u001b[39m):\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/zheng/Desktop/ETH%20Third%20Semester/AML/AML%20projects/Second/ViktorExploration.ipynb#X22sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m     signals, info \u001b[39m=\u001b[39m nk\u001b[39m.\u001b[39mecg_process(signal, sampling_rate\u001b[39m=\u001b[39msampling_rate)\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/zheng/Desktop/ETH%20Third%20Semester/AML/AML%20projects/Second/ViktorExploration.ipynb#X22sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m     features \u001b[39m=\u001b[39m nk\u001b[39m.\u001b[39mecg_intervalrelated(signals, sampling_rate\u001b[39m=\u001b[39msampling_rate)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/zheng/Desktop/ETH%20Third%20Semester/AML/AML%20projects/Second/ViktorExploration.ipynb#X22sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m features\n",
      "File \u001b[1;32mc:\\Users\\zheng\\anaconda3\\Lib\\site-packages\\neurokit2\\ecg\\ecg_intervalrelated.py:63\u001b[0m, in \u001b[0;36mecg_intervalrelated\u001b[1;34m(data, sampling_rate)\u001b[0m\n\u001b[0;32m     61\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(rate_cols) \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m     62\u001b[0m     intervals\u001b[39m.\u001b[39mupdate(_ecg_intervalrelated_formatinput(data))\n\u001b[1;32m---> 63\u001b[0m     intervals\u001b[39m.\u001b[39mupdate(_ecg_intervalrelated_hrv(data, sampling_rate))\n\u001b[0;32m     64\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     65\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m     66\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mNeuroKit error: ecg_intervalrelated(): Wrong input,\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m     67\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mwe couldn\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt extract heart rate. Please make sure\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m     68\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39myour DataFrame contains an `ECG_Rate` column.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m     69\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\zheng\\anaconda3\\Lib\\site-packages\\neurokit2\\ecg\\ecg_intervalrelated.py:128\u001b[0m, in \u001b[0;36m_ecg_intervalrelated_hrv\u001b[1;34m(data, sampling_rate, output)\u001b[0m\n\u001b[0;32m    125\u001b[0m rpeaks \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mwhere(data[\u001b[39m\"\u001b[39m\u001b[39mECG_R_Peaks\u001b[39m\u001b[39m\"\u001b[39m]\u001b[39m.\u001b[39mvalues)[\u001b[39m0\u001b[39m]\n\u001b[0;32m    126\u001b[0m rpeaks \u001b[39m=\u001b[39m {\u001b[39m\"\u001b[39m\u001b[39mECG_R_Peaks\u001b[39m\u001b[39m\"\u001b[39m: rpeaks}\n\u001b[1;32m--> 128\u001b[0m results \u001b[39m=\u001b[39m hrv(rpeaks, sampling_rate\u001b[39m=\u001b[39msampling_rate)\n\u001b[0;32m    129\u001b[0m \u001b[39mfor\u001b[39;00m column \u001b[39min\u001b[39;00m results\u001b[39m.\u001b[39mcolumns:\n\u001b[0;32m    130\u001b[0m     \u001b[39m# Add and convert to float\u001b[39;00m\n\u001b[0;32m    131\u001b[0m     output[column] \u001b[39m=\u001b[39m results[[column]]\u001b[39m.\u001b[39mvalues\n",
      "File \u001b[1;32mc:\\Users\\zheng\\anaconda3\\Lib\\site-packages\\neurokit2\\hrv\\hrv.py:107\u001b[0m, in \u001b[0;36mhrv\u001b[1;34m(peaks, sampling_rate, show, **kwargs)\u001b[0m\n\u001b[0;32m    105\u001b[0m out\u001b[39m.\u001b[39mappend(hrv_time(peaks, sampling_rate\u001b[39m=\u001b[39msampling_rate))\n\u001b[0;32m    106\u001b[0m out\u001b[39m.\u001b[39mappend(hrv_frequency(peaks, sampling_rate\u001b[39m=\u001b[39msampling_rate))\n\u001b[1;32m--> 107\u001b[0m out\u001b[39m.\u001b[39mappend(hrv_nonlinear(peaks, sampling_rate\u001b[39m=\u001b[39msampling_rate))\n\u001b[0;32m    109\u001b[0m \u001b[39m# Compute RSA if rsp data is available\u001b[39;00m\n\u001b[0;32m    110\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(peaks, pd\u001b[39m.\u001b[39mDataFrame):\n",
      "File \u001b[1;32mc:\\Users\\zheng\\anaconda3\\Lib\\site-packages\\neurokit2\\hrv\\hrv_nonlinear.py:245\u001b[0m, in \u001b[0;36mhrv_nonlinear\u001b[1;34m(peaks, sampling_rate, show, **kwargs)\u001b[0m\n\u001b[0;32m    242\u001b[0m out[\u001b[39m\"\u001b[39m\u001b[39mRCMSEn\u001b[39m\u001b[39m\"\u001b[39m], _ \u001b[39m=\u001b[39m entropy_multiscale(rri, dimension\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m, tolerance\u001b[39m=\u001b[39mtolerance, method\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mRCMSEn\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    244\u001b[0m out[\u001b[39m\"\u001b[39m\u001b[39mCD\u001b[39m\u001b[39m\"\u001b[39m], _ \u001b[39m=\u001b[39m fractal_correlation(rri, delay\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m, dimension\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m--> 245\u001b[0m out[\u001b[39m\"\u001b[39m\u001b[39mHFD\u001b[39m\u001b[39m\"\u001b[39m], _ \u001b[39m=\u001b[39m fractal_higuchi(rri, k_max\u001b[39m=\u001b[39m\u001b[39m10\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    246\u001b[0m out[\u001b[39m\"\u001b[39m\u001b[39mKFD\u001b[39m\u001b[39m\"\u001b[39m], _ \u001b[39m=\u001b[39m fractal_katz(rri)\n\u001b[0;32m    247\u001b[0m out[\u001b[39m\"\u001b[39m\u001b[39mLZC\u001b[39m\u001b[39m\"\u001b[39m], _ \u001b[39m=\u001b[39m complexity_lempelziv(rri, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\zheng\\anaconda3\\Lib\\site-packages\\neurokit2\\complexity\\fractal_higuchi.py:93\u001b[0m, in \u001b[0;36mfractal_higuchi\u001b[1;34m(signal, k_max, show, **kwargs)\u001b[0m\n\u001b[0;32m     90\u001b[0m     k_values \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marange(\u001b[39m1\u001b[39m, k_max \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m)\n\u001b[0;32m     91\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     92\u001b[0m     \u001b[39m# Compute Higuchi\u001b[39;00m\n\u001b[1;32m---> 93\u001b[0m     slope, intercept, info \u001b[39m=\u001b[39m _complexity_k_slope(k_max, signal)\n\u001b[0;32m     94\u001b[0m     k_values \u001b[39m=\u001b[39m info[\u001b[39m\"\u001b[39m\u001b[39mk_values\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[0;32m     95\u001b[0m     average_values \u001b[39m=\u001b[39m info[\u001b[39m\"\u001b[39m\u001b[39maverage_values\u001b[39m\u001b[39m\"\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\zheng\\anaconda3\\Lib\\site-packages\\neurokit2\\complexity\\optim_complexity_k.py:154\u001b[0m, in \u001b[0;36m_complexity_k_slope\u001b[1;34m(kmax, signal, k_number)\u001b[0m\n\u001b[0;32m    151\u001b[0m average_values \u001b[39m=\u001b[39m vectorized_Lk(k_values, signal)\n\u001b[0;32m    153\u001b[0m \u001b[39m# Slope of best-fit line through points (slope equal to FD)\u001b[39;00m\n\u001b[1;32m--> 154\u001b[0m slope, intercept \u001b[39m=\u001b[39m \u001b[39m-\u001b[39mnp\u001b[39m.\u001b[39mpolyfit(np\u001b[39m.\u001b[39mlog(k_values), np\u001b[39m.\u001b[39mlog(average_values), \u001b[39m1\u001b[39m)\n\u001b[0;32m    155\u001b[0m \u001b[39mreturn\u001b[39;00m slope, intercept, {\u001b[39m\"\u001b[39m\u001b[39mk_values\u001b[39m\u001b[39m\"\u001b[39m: k_values, \u001b[39m\"\u001b[39m\u001b[39maverage_values\u001b[39m\u001b[39m\"\u001b[39m: average_values}\n",
      "File \u001b[1;32m<__array_function__ internals>:200\u001b[0m, in \u001b[0;36mpolyfit\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\zheng\\anaconda3\\Lib\\site-packages\\numpy\\lib\\polynomial.py:668\u001b[0m, in \u001b[0;36mpolyfit\u001b[1;34m(x, y, deg, rcond, full, w, cov)\u001b[0m\n\u001b[0;32m    666\u001b[0m scale \u001b[39m=\u001b[39m NX\u001b[39m.\u001b[39msqrt((lhs\u001b[39m*\u001b[39mlhs)\u001b[39m.\u001b[39msum(axis\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m))\n\u001b[0;32m    667\u001b[0m lhs \u001b[39m/\u001b[39m\u001b[39m=\u001b[39m scale\n\u001b[1;32m--> 668\u001b[0m c, resids, rank, s \u001b[39m=\u001b[39m lstsq(lhs, rhs, rcond)\n\u001b[0;32m    669\u001b[0m c \u001b[39m=\u001b[39m (c\u001b[39m.\u001b[39mT\u001b[39m/\u001b[39mscale)\u001b[39m.\u001b[39mT  \u001b[39m# broadcast scale coefficients\u001b[39;00m\n\u001b[0;32m    671\u001b[0m \u001b[39m# warn on rank reduction, which indicates an ill conditioned matrix\u001b[39;00m\n",
      "File \u001b[1;32m<__array_function__ internals>:200\u001b[0m, in \u001b[0;36mlstsq\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\zheng\\anaconda3\\Lib\\site-packages\\numpy\\linalg\\linalg.py:2285\u001b[0m, in \u001b[0;36mlstsq\u001b[1;34m(a, b, rcond)\u001b[0m\n\u001b[0;32m   2282\u001b[0m \u001b[39mif\u001b[39;00m n_rhs \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m   2283\u001b[0m     \u001b[39m# lapack can't handle n_rhs = 0 - so allocate the array one larger in that axis\u001b[39;00m\n\u001b[0;32m   2284\u001b[0m     b \u001b[39m=\u001b[39m zeros(b\u001b[39m.\u001b[39mshape[:\u001b[39m-\u001b[39m\u001b[39m2\u001b[39m] \u001b[39m+\u001b[39m (m, n_rhs \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m), dtype\u001b[39m=\u001b[39mb\u001b[39m.\u001b[39mdtype)\n\u001b[1;32m-> 2285\u001b[0m x, resids, rank, s \u001b[39m=\u001b[39m gufunc(a, b, rcond, signature\u001b[39m=\u001b[39msignature, extobj\u001b[39m=\u001b[39mextobj)\n\u001b[0;32m   2286\u001b[0m \u001b[39mif\u001b[39;00m m \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m   2287\u001b[0m     x[\u001b[39m.\u001b[39m\u001b[39m.\u001b[39m\u001b[39m.\u001b[39m] \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\zheng\\anaconda3\\Lib\\site-packages\\numpy\\linalg\\linalg.py:101\u001b[0m, in \u001b[0;36m_raise_linalgerror_lstsq\u001b[1;34m(err, flag)\u001b[0m\n\u001b[0;32m    100\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_raise_linalgerror_lstsq\u001b[39m(err, flag):\n\u001b[1;32m--> 101\u001b[0m     \u001b[39mraise\u001b[39;00m LinAlgError(\u001b[39m\"\u001b[39m\u001b[39mSVD did not converge in Linear Least Squares\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[1;31mLinAlgError\u001b[0m: SVD did not converge in Linear Least Squares"
     ]
    }
   ],
   "source": [
    "def process_ecg(signal, sampling_rate=300):\n",
    "    signals, info = nk.ecg_process(signal, sampling_rate=sampling_rate)\n",
    "    features = nk.ecg_intervalrelated(signals, sampling_rate=sampling_rate)\n",
    "    return features\n",
    "\n",
    "\n",
    "neurokit_features_df = pd.DataFrame()\n",
    "# Iterate over each row in the dataframe\n",
    "for i in range(padded.shape[0]):\n",
    "    row = padded[i]\n",
    "    # Process each row and extract features\n",
    "    features = process_ecg(row)    \n",
    "    # Append the features to the features_df\n",
    "    neurokit_features_df = pd.concat([neurokit_features_df, features], ignore_index=True)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = nk.data(\"bio_resting_5min_100hz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ECG</th>\n",
       "      <th>PPG</th>\n",
       "      <th>RSP</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.003766</td>\n",
       "      <td>-0.102539</td>\n",
       "      <td>0.494652</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.017466</td>\n",
       "      <td>-0.103760</td>\n",
       "      <td>0.502483</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.015679</td>\n",
       "      <td>-0.107422</td>\n",
       "      <td>0.511102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.001598</td>\n",
       "      <td>-0.110855</td>\n",
       "      <td>0.518791</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.002483</td>\n",
       "      <td>-0.112610</td>\n",
       "      <td>0.528669</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29995</th>\n",
       "      <td>-0.045986</td>\n",
       "      <td>-0.135498</td>\n",
       "      <td>0.981111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29996</th>\n",
       "      <td>-0.107841</td>\n",
       "      <td>-0.155334</td>\n",
       "      <td>0.978866</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29997</th>\n",
       "      <td>-0.058274</td>\n",
       "      <td>-0.173721</td>\n",
       "      <td>0.976366</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29998</th>\n",
       "      <td>-0.056472</td>\n",
       "      <td>-0.192108</td>\n",
       "      <td>0.972202</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29999</th>\n",
       "      <td>-0.044856</td>\n",
       "      <td>-0.209579</td>\n",
       "      <td>0.969100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>30000 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            ECG       PPG       RSP\n",
       "0      0.003766 -0.102539  0.494652\n",
       "1     -0.017466 -0.103760  0.502483\n",
       "2     -0.015679 -0.107422  0.511102\n",
       "3     -0.001598 -0.110855  0.518791\n",
       "4      0.002483 -0.112610  0.528669\n",
       "...         ...       ...       ...\n",
       "29995 -0.045986 -0.135498  0.981111\n",
       "29996 -0.107841 -0.155334  0.978866\n",
       "29997 -0.058274 -0.173721  0.976366\n",
       "29998 -0.056472 -0.192108  0.972202\n",
       "29999 -0.044856 -0.209579  0.969100\n",
       "\n",
       "[30000 rows x 3 columns]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df, info = nk.ecg_process(data[\"ECG\"], sampling_rate=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ECG_Rate_Mean</th>\n",
       "      <th>HRV_MeanNN</th>\n",
       "      <th>HRV_SDNN</th>\n",
       "      <th>HRV_SDANN1</th>\n",
       "      <th>HRV_SDNNI1</th>\n",
       "      <th>HRV_SDANN2</th>\n",
       "      <th>HRV_SDNNI2</th>\n",
       "      <th>HRV_SDANN5</th>\n",
       "      <th>HRV_SDNNI5</th>\n",
       "      <th>HRV_RMSSD</th>\n",
       "      <th>...</th>\n",
       "      <th>HRV_LZC</th>\n",
       "      <th>HRV_DFA_alpha2</th>\n",
       "      <th>HRV_MFDFA_alpha2_Width</th>\n",
       "      <th>HRV_MFDFA_alpha2_Peak</th>\n",
       "      <th>HRV_MFDFA_alpha2_Mean</th>\n",
       "      <th>HRV_MFDFA_alpha2_Max</th>\n",
       "      <th>HRV_MFDFA_alpha2_Delta</th>\n",
       "      <th>HRV_MFDFA_alpha2_Asymmetry</th>\n",
       "      <th>HRV_MFDFA_alpha2_Fluctuation</th>\n",
       "      <th>HRV_MFDFA_alpha2_Increment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>86.392105</td>\n",
       "      <td>[[694.7563805104409]]</td>\n",
       "      <td>[[49.03604322104105]]</td>\n",
       "      <td>[[7.277184736458611]]</td>\n",
       "      <td>[[48.83361026572851]]</td>\n",
       "      <td>[[nan]]</td>\n",
       "      <td>[[nan]]</td>\n",
       "      <td>[[nan]]</td>\n",
       "      <td>[[nan]]</td>\n",
       "      <td>[[38.83776632381496]]</td>\n",
       "      <td>...</td>\n",
       "      <td>[[0.8731238852455482]]</td>\n",
       "      <td>[[0.74458349456187]]</td>\n",
       "      <td>[[0.12025402871845392]]</td>\n",
       "      <td>[[0.6260707670636847]]</td>\n",
       "      <td>[[0.6861977814229117]]</td>\n",
       "      <td>[[1.0034826024405372]]</td>\n",
       "      <td>[[-0.09163628837599647]]</td>\n",
       "      <td>[[0.0]]</td>\n",
       "      <td>[[2.6738753815679935e-05]]</td>\n",
       "      <td>[[0.0018592659897405457]]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 92 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  ECG_Rate_Mean             HRV_MeanNN               HRV_SDNN  \\\n",
       "0     86.392105  [[694.7563805104409]]  [[49.03604322104105]]   \n",
       "\n",
       "              HRV_SDANN1             HRV_SDNNI1 HRV_SDANN2 HRV_SDNNI2  \\\n",
       "0  [[7.277184736458611]]  [[48.83361026572851]]    [[nan]]    [[nan]]   \n",
       "\n",
       "  HRV_SDANN5 HRV_SDNNI5              HRV_RMSSD  ...                 HRV_LZC  \\\n",
       "0    [[nan]]    [[nan]]  [[38.83776632381496]]  ...  [[0.8731238852455482]]   \n",
       "\n",
       "         HRV_DFA_alpha2   HRV_MFDFA_alpha2_Width   HRV_MFDFA_alpha2_Peak  \\\n",
       "0  [[0.74458349456187]]  [[0.12025402871845392]]  [[0.6260707670636847]]   \n",
       "\n",
       "    HRV_MFDFA_alpha2_Mean    HRV_MFDFA_alpha2_Max    HRV_MFDFA_alpha2_Delta  \\\n",
       "0  [[0.6861977814229117]]  [[1.0034826024405372]]  [[-0.09163628837599647]]   \n",
       "\n",
       "  HRV_MFDFA_alpha2_Asymmetry HRV_MFDFA_alpha2_Fluctuation  \\\n",
       "0                    [[0.0]]   [[2.6738753815679935e-05]]   \n",
       "\n",
       "  HRV_MFDFA_alpha2_Increment  \n",
       "0  [[0.0018592659897405457]]  \n",
       "\n",
       "[1 rows x 92 columns]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nk.ecg_intervalrelated(df, sampling_rate=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
